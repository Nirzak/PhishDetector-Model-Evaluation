{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # imports a fast numerical programming library\n",
    "import scipy as sp #imports stats functions, amongst other things\n",
    "import matplotlib as mpl # this actually imports matplotlib\n",
    "import matplotlib.cm as cm #allows us easy access to colormaps\n",
    "import matplotlib.pyplot as plt #sets up plotting under plt\n",
    "import pandas as pd #lets us handle data as dataframes\n",
    "from pandas import DataFrame\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running neural networks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:921: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[1246  209]\n",
      " [ 155 1707]]\n",
      "Confusion Matrix Shape:  (2, 2)\n",
      "TP\tFP\tFN\tTN\n",
      "1246.0\t155.0\t209.0\t1707.0\n",
      "Sensitivity: 0.86\n",
      "Specificity: 0.92\n",
      "1707.0\t209.0\t155.0\t1246.0\n",
      "Sensitivity: 0.92\n",
      "Specificity: 0.86\n",
      "The F1 score:  0.9036527263102171\n",
      "Accuracy:  0.8902622851974676\n",
      "runtime = 9.046810388565063 seconds\n",
      "\n",
      "running random forests...\n",
      "Confusion Matrix: \n",
      "[[1293  162]\n",
      " [ 182 1680]]\n",
      "Confusion Matrix Shape:  (2, 2)\n",
      "TP\tFP\tFN\tTN\n",
      "1293.0\t182.0\t162.0\t1680.0\n",
      "Sensitivity: 0.89\n",
      "Specificity: 0.9\n",
      "1680.0\t162.0\t182.0\t1293.0\n",
      "Sensitivity: 0.9\n",
      "Specificity: 0.89\n",
      "The F1 score:  0.9071274298056156\n",
      "Accuracy:  0.8962918299668375\n",
      "runtime = 0.1296522617340088 seconds\n",
      "\n",
      "running support vector machines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[1249  206]\n",
      " [ 131 1731]]\n",
      "Confusion Matrix Shape:  (2, 2)\n",
      "TP\tFP\tFN\tTN\n",
      "1249.0\t131.0\t206.0\t1731.0\n",
      "Sensitivity: 0.86\n",
      "Specificity: 0.93\n",
      "1731.0\t206.0\t131.0\t1249.0\n",
      "Sensitivity: 0.93\n",
      "Specificity: 0.86\n",
      "The F1 score:  0.9112924453803632\n",
      "Accuracy:  0.8984021706361169\n",
      "runtime = 1.7762465476989746 seconds\n",
      "runtime = 10.953709363937378 seconds\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import time\n",
    "\n",
    "def calculate_metrics(y_test,Y_predicted):\n",
    "\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test,Y_predicted)\n",
    "    #print \"accuracy = \"+str(round(accuracy * 100,2))+\"%\"\n",
    "\n",
    "    confusion_mat = confusion_matrix(y_test,Y_predicted)\n",
    "\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(confusion_mat)\n",
    "    print(\"Confusion Matrix Shape: \", confusion_mat.shape)\n",
    "\n",
    "    print(\"TP\\tFP\\tFN\\tTN\")\n",
    "    for i in range(confusion_mat.shape[0]):\n",
    "        TP = round(float(confusion_mat[i,i]),2)  # correctly labeled as i\n",
    "        FP = round(float(confusion_mat[:,i].sum()),2) - TP  # incorrectly labeled as i\n",
    "        FN = round(float(confusion_mat[i,:].sum()),2) - TP  # incorrectly labeled as non-i\n",
    "        TN = round(float(confusion_mat.sum().sum()),2) - TP - FP - FN\n",
    "        print(str(TP)+\"\\t\"+str(FP)+\"\\t\"+str(FN)+\"\\t\"+str(TN)),\n",
    "        sensitivity = round(TP / (TP + FN),2)\n",
    "        specificity = round(TN / (TN + FP),2)\n",
    "        print(\"Sensitivity: \"+str(sensitivity))\n",
    "        print(\"Specificity: \"+str(specificity))\n",
    "\n",
    "\n",
    "    f_score = metrics.f1_score(y_test,Y_predicted)\n",
    "    print(\"The F1 score: \", f_score)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    \n",
    "\n",
    "def neural_network(dataset,class_labels,test_size):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    X = pd.read_csv(dataset)\n",
    "    Y = pd.read_csv(class_labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size= test_size, random_state=42) \n",
    "    model = MLPClassifier(hidden_layer_sizes=(100), activation='logistic',random_state = 42)\n",
    "    model.fit(X_train,y_train)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    return y_test,Y_predicted\n",
    "\n",
    "\n",
    "def random_forests(dataset,class_labels,test_size):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn import metrics\n",
    "    X = pd.read_csv(dataset)\n",
    "    Y = pd.read_csv(class_labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size= test_size, random_state=42)   \n",
    "    model = RandomForestClassifier(n_estimators = 5, criterion = 'entropy',random_state = 42)\n",
    "    model.fit(X_train,y_train)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    return y_test,Y_predicted\n",
    "\n",
    "def support_vector_machines(dataset,class_labels,test_size):\n",
    "    import numpy as np\n",
    "    from sklearn import svm\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = pd.read_csv(dataset)\n",
    "    Y = pd.read_csv(class_labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size= test_size, random_state=42)  \n",
    "    # 'rbf' value is the gaussian kernel, 'C' is the coefficient used for regularization during training\n",
    "    model = svm.SVC(kernel='rbf',C=2.0)\n",
    "    model.fit(X_train,y_train)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    return y_test,Y_predicted\n",
    "\n",
    "def main():\n",
    "\n",
    "    dataset = \"dataset1.csv\"\n",
    "    class_labels = \"target_labels.csv\"\n",
    "    test_size = 0.3\n",
    "    print(\"\\nrunning neural networks...\")\n",
    "    start_time = time.time()\n",
    "    y_test,Y_predicted = neural_network(dataset,class_labels,test_size)\n",
    "    calculate_metrics(y_test,Y_predicted)\n",
    "    end_time = time.time()\n",
    "    print(\"runtime = \"+str(end_time - start_time)+\" seconds\")\n",
    "    print(\"\\nrunning random forests...\")\n",
    "    start_time = time.time()\n",
    "    y_test,Y_predicted = random_forests(dataset,class_labels,test_size)\n",
    "    calculate_metrics(y_test,Y_predicted)\n",
    "    end_time = time.time()\n",
    "    print(\"runtime = \"+str(end_time - start_time)+\" seconds\")\n",
    "\n",
    "    print(\"\\nrunning support vector machines...\")\n",
    "    start_time = time.time()\n",
    "    y_test,Y_predicted = support_vector_machines(dataset,class_labels,test_size)\n",
    "    calculate_metrics(y_test,Y_predicted)\n",
    "    end_time = time.time()\n",
    "    print(\"runtime = \"+str(end_time - start_time)+\" seconds\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    end_time = time.time()\n",
    "    print(\"runtime = \"+str(end_time - start_time)+\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running neural networks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:921: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import time\n",
    "\n",
    "def calculate_metrics(y_test,Y_predicted):\n",
    "\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test,Y_predicted)\n",
    "    #print \"accuracy = \"+str(round(accuracy * 100,2))+\"%\"\n",
    "\n",
    "    confusion_mat = confusion_matrix(y_test,Y_predicted)\n",
    "\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(confusion_mat)\n",
    "    print(\"Confusion Matrix Shape: \", confusion_mat.shape)\n",
    "\n",
    "    print(\"TP\\tFP\\tFN\\tTN\\tSensitivity\\tSpecificity\")\n",
    "    for i in range(confusion_mat.shape[0]):\n",
    "        TP = round(float(confusion_mat[i,i]),2)  # correctly labeled as i\n",
    "        FP = round(float(confusion_mat[:,i].sum()),2) - TP  # incorrectly labeled as i\n",
    "        FN = round(float(confusion_mat[i,:].sum()),2) - TP  # incorrectly labeled as non-i\n",
    "        TN = round(float(confusion_mat.sum().sum()),2) - TP - FP - FN\n",
    "        print(str(TP)+\"\\t\"+str(FP)+\"\\t\"+str(FN)+\"\\t\"+str(TN)),\n",
    "        sensitivity = round(TP / (TP + FN),2)\n",
    "        specificity = round(TN / (TN + FP),2)\n",
    "        print(\"Sensitivity: \"+str(sensitivity))\n",
    "        print(\"Specificity: \"+str(specificity))\n",
    "\n",
    "\n",
    "    f_score = metrics.f1_score(y_test,Y_predicted)\n",
    "    print(\"The F1 score: \", f_score)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    \n",
    "\n",
    "def neural_network(dataset,class_labels,test_size):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    X = pd.read_csv(dataset)\n",
    "    Y = pd.read_csv(class_labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size= test_size, random_state=42) \n",
    "    model = MLPClassifier(hidden_layer_sizes=(100), activation='logistic',random_state = 42)\n",
    "    model.fit(X_train,y_train)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    return y_test,Y_predicted\n",
    "\n",
    "\n",
    "def random_forests(dataset,class_labels,test_size):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn import metrics\n",
    "    X = pd.read_csv(dataset)\n",
    "    Y = pd.read_csv(class_labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size= test_size, random_state=42)   \n",
    "    model = RandomForestClassifier(n_estimators = 5, criterion = 'entropy',random_state = 42)\n",
    "    model.fit(X_train,y_train)\n",
    "    filename = 'finalized_model.pkl'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    return y_test,Y_predicted\n",
    "\n",
    "def support_vector_machines(dataset,class_labels,test_size):\n",
    "    import numpy as np\n",
    "    from sklearn import svm\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = pd.read_csv(dataset)\n",
    "    Y = pd.read_csv(class_labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size= test_size, random_state=42)  \n",
    "    # 'rbf' value is the gaussian kernel, 'C' is the coefficient used for regularization during training\n",
    "    model = svm.SVC(kernel='rbf',C=2.0)\n",
    "    model.fit(X_train,y_train)\n",
    "    Y_predicted = model.predict(X_test)\n",
    "    return y_test,Y_predicted\n",
    "\n",
    "def main():\n",
    "\n",
    "    dataset = \"dataset2.csv\"\n",
    "    class_labels = \"target_labels.csv\"\n",
    "    test_size = 0.3\n",
    "    print(\"\\nrunning neural networks...\")\n",
    "    start_time = time.time()\n",
    "    y_test,Y_predicted = neural_network(dataset,class_labels,test_size)\n",
    "    calculate_metrics(y_test,Y_predicted)\n",
    "    end_time = time.time()\n",
    "    print(\"runtime = \"+str(end_time - start_time)+\" seconds\")\n",
    "    print(\"\\nrunning random forests...\")\n",
    "    start_time = time.time()\n",
    "    y_test,Y_predicted = random_forests(dataset,class_labels,test_size)\n",
    "    calculate_metrics(y_test,Y_predicted)\n",
    "    end_time = time.time()\n",
    "    print(\"runtime = \"+str(end_time - start_time)+\" seconds\")\n",
    "\n",
    "    print(\"\\nrunning support vector machines...\")\n",
    "    start_time = time.time()\n",
    "    y_test,Y_predicted = support_vector_machines(dataset,class_labels,test_size)\n",
    "    calculate_metrics(y_test,Y_predicted)\n",
    "    end_time = time.time()\n",
    "    print(\"runtime = \"+str(end_time - start_time)+\" seconds\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    end_time = time.time()\n",
    "    print(\"runtime = \"+str(end_time - start_time)+\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
